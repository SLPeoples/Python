In this chapter, you go beyond the single machine-learning models you’ve
seen until now and explore the power of ensembles, groups of models that
can outperform single models. Ensembles work like the collective intelligence
of crowds, using pooled information to make better predictions. The basic
idea is that a group of nonperforming algorithms can produce better results
than a single well-trained model.
Maybe you’ve participated in one of those games that ask you to guess the
number of sweets in a jar at parties or fairs. Even though a single person
has a slim chance of guessing the right number, various experiments have
confirmed that if you take the wrong answers of a large number of game
participants and average them, you can get close to the right answer! Such
incredible shared group knowledge (the wisdom of crowds) is possible
because wrong answers tend to distribute around the true one. By taking
a mean or median of these wrong answers, you get the direction of the
right answer.
You can use this technique to win games by listening carefully to others’
answers when participating in such games before providing your informed
answer. Of course, you can employ the technique in ways that are more
practical. In data science projects involving complex predictions, you can
leverage the wisdom of various machine-learning algorithms and become
more precise and accurate at predictions than you can when using a single
algorithm. This chapter creates a process you can use to leverage the power
of many different algorithms to obtain a better single answer.
You don’t have to type the source code for this chapter manually. In fact, it’s a
lot easier if you use the downloadable source (see the Introduction for download
instructions). The source code for this chapter appears in the P4DS4D;
20; Understanding the Power of the Many.ipynb source code file.